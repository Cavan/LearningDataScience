{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 , 0  found so far\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-029fa599ce1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m     '''\n\u001b[0;32m     73\u001b[0m     \u001b[1;31m#Plot the number of books related to data by year\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     year_counts = Counter(get_year(book) for book in books \n\u001b[0m\u001b[0;32m     75\u001b[0m                          if get_year(book) <= 2019)\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\DataScienceFromScratch\\lib\\collections.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\DataScienceFromScratch\\lib\\collections.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m                 \u001b[0mself_get\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-029fa599ce1c>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m((book,))\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;31m#Plot the number of books related to data by year\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     year_counts = Counter(get_year(book) for book in books \n\u001b[1;32m---> 75\u001b[1;33m                          if get_year(book) <= 2019)\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0myears\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-029fa599ce1c>\u001b[0m in \u001b[0;36mget_year\u001b[1;34m(book)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \"\"\"book[\"date\"] looks like 'May2019' so we need to extract the number \n\u001b[0;32m     35\u001b[0m     from the string\"\"\"\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbook\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# Scrape a website searching for books related to the subject data\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#Parse out the data from the site\n",
    "def book_info(article):\n",
    "    \"\"\"Given a BeautifulSoup <article> Tag extract the books \n",
    "        details and return in a dict\"\"\"\n",
    "    title = article.find('p', 'title').a.text\n",
    "    author_name = article.find('p', 'note').text\n",
    "    authors = [x.strip() for x in re.sub(\"^By \", \"\", author_name).split(\",\")]\n",
    "    published = article.find('p', 'note date2').text\n",
    "    # Publishing date sanitation\n",
    "    cleaned_date = re.sub(' +', '', published)\n",
    "    remove_newline = re.sub('\\n', '', cleaned_date)\n",
    "    extracted_date = [x.strip() for x in re.sub(\"^ReleaseDate:\", \"\", cleaned_date).split(\",\")]\n",
    "    \n",
    "    return {\n",
    "        \"title\" : title,\n",
    "        \"authors\" : authors,\n",
    "        \"date\" : extracted_date\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "#Plot the number of books being published each year\n",
    "def get_year(book):\n",
    "    \"\"\"book[\"date\"] looks like 'May2019' so we need to extract the number \n",
    "    from the string\"\"\"\n",
    "    return [int(s) for s in book[\"date\"].split() if s.isdigit()]\n",
    "\n",
    "\n",
    "base_url = \"https://ssearch.oreilly.com/?i=1;m_Sort=searchDate;\"\n",
    "#headers={\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36\"}\n",
    "\n",
    "csv_columns = ['title', 'authors', 'date']\n",
    "\n",
    "books = []\n",
    "\n",
    "NUM_PAGES = 100 #Total number of pages to scrape\n",
    "\n",
    "for page_num in range(1, NUM_PAGES + 1):\n",
    "    print \"Scraping page\", page_num, \",\", len(books), \" found so far\"\n",
    "    if page_num == 1:\n",
    "        url = base_url + \"q=data;q1=Books;x1=t1&act=pg_1\"\n",
    "    else:\n",
    "        url = base_url + \"page=%s;q=data;q1=Books;x1=t1&act=pg_%s\" % (page_num, page_num)\n",
    "\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html5lib')\n",
    "    \n",
    "    for article in soup('article', 'product-result'):\n",
    "        books.append(book_info(article))\n",
    "    '''    \n",
    "    # Sleep for 10 seconds\n",
    "    print(books[page_num])\n",
    "    \n",
    "    csvData = [books[page_num]]\n",
    "    \n",
    "    with open('data_books2.csv', 'a') as csvFile:\n",
    "        writer = csv.DictWriter(csvFile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for data in csvData:\n",
    "            writer.writerow(data)\n",
    "            \n",
    "    csvFile.close()\n",
    "    '''\n",
    "    #Plot the number of books related to data by year\n",
    "    year_counts = Counter(get_year(book) for book in books \n",
    "                         if get_year(book) <= 2019)\n",
    "    \n",
    "    years = sorted(year_counts)\n",
    "    book_counts = [year_counts[year] for year in years]\n",
    "    plt.plot(years, book_counts)\n",
    "    plt.ylabel(\"# of data books\")\n",
    "    plt.title(\"Data is Big\")\n",
    "    plt.show()\n",
    "    \n",
    "                                                \n",
    "    \n",
    "    sleep(10)\n",
    "\n",
    "\n",
    "#Structure layout of website\n",
    "'''\n",
    "<article class=\"result\">\n",
    "            \n",
    "\n",
    "            <a class=\"learn-more\"\n",
    "            href=\"https://www.oreilly.com/pub/e/3289\">Learn more</a>\n",
    "            <p class=\"title\">\n",
    "              \n",
    "                <a href=\"https://www.oreilly.com/pub/e/3289\"> 2015 Data\n",
    "                Preview: Spark, Data Visualization, YARN, and More - O'Reilly\n",
    "                Media... </a>\n",
    "              \n",
    "            </p>\n",
    "              <p class=\"note\">By Alistair Croll</p>\n",
    "  \n",
    "                <p class=\"note date2\">Publish Date: \n",
    "\n",
    "                        July 29, 2016\n",
    "                   \n",
    "</p>\n",
    "              <p class=\"description\"> In February, Big Data's biggest event\n",
    "              comes to the Bay Area. Get a sneak peek with this free online\n",
    "              conference, featuring many of Strata's most sought-after\n",
    "              speakers and hottest topics. About Alistair Croll Alistair has\n",
    "              been an entrepreneur... </p>\n",
    "\n",
    "            </article>\n",
    "\n",
    "'''\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
